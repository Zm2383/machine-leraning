{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361798bd",
   "metadata": {},
   "source": [
    "# 1. Import the spam dataset and print the first six rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b4cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import os\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee117970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/apple/Downloads/')\n",
    "\n",
    "df= pd.read_csv('spam_dataset.csv')\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1f6cb",
   "metadata": {},
   "source": [
    "# 2.The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2889bc",
   "metadata": {},
   "source": [
    "word_freq_free: Sometimes people are more likely to be attracted to any free things. But this can be a spam and people may be deceived into wasting money. So \"free\" can be predictor.\n",
    "word_freq_credit:Combined with their own experience, there are many spam applications for credit cards.\n",
    "word_freq_address: If emails say that your address should be provided, or mention any address requirement, it can be a spam. So it can be another predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88304674",
   "metadata": {},
   "source": [
    "# 3. Visualize the univariate distribution of each of the variables in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de047dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['word_freq_address:'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6acb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['word_freq_credit:'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78185a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['word_freq_free:'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febcdf6",
   "metadata": {},
   "source": [
    "# 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250404e4",
   "metadata": {},
   "source": [
    "Model: K Nearest Neighbors(KNN), logistic regression, penalized logistic regression, Support Vector Machines(SVM), decision tree, bagged trees, random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f01c90",
   "metadata": {},
   "source": [
    "# 5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14485500",
   "metadata": {},
   "source": [
    "The importance of training data: in machine learning, \"Training data is the main and most important data to help machine learn and make predictions. This training data set is used to develop algorithms and about 75% of the total data used in the project. Training algorithms are very important on training data sets that are separate and different from the test set, because we need to know the data set (model) Accuracy. Not using training data may lead to poor generalization ability. The reason for dividing the data into training data set and test data set is that we want to generate a training model, which can be used for new unknown data set (model) to predict the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb1f42",
   "metadata": {},
   "source": [
    "# 6. What is k-fold cross validation and what do we use it for?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf86c5",
   "metadata": {},
   "source": [
    "K-fold cross validation is a resampling technique. \"This is to randomly divide the samples into k sets of approximately equal size. Fit the model with all samples except the first subset (called the first fold).\" The model is used to predict the retained samples and evaluate the performance indicators. The first subset is returned to the training set, the process repeats, the second subset remains unchanged, and so on. \"From textbook * application prediction modeling. What is the purpose of our use of it:\" summarize the K resampling estimates of performance (usually with mean and standard error) and use them to understand the relationship between tuning parameters and model practicability. We use k-folding cross validation as a program to estimate the skills of the model on new data. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9291a52",
   "metadata": {},
   "source": [
    "# 7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21913f6",
   "metadata": {},
   "source": [
    "Stratification is the process of rearranging data to ensure that each fold is a good representation of the whole. Layered folding is a variant of kfold. Hierarchical folding shuffles the data, and then splits the data into n_ Split part. Before splitting, it shuffles the data only and always once. Layering ensures that the relative class frequency ch fold in EA reflects the relative class frequency on the whole data set. Multiple use of stratified k-folding and random time can reduce the variance. Generally speaking, k-fold CV divides the data set into k folds, and hierarchical k-fold ensures that each fold of the data set has the same proportion of observations and has a given label. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaa3ab",
   "metadata": {},
   "source": [
    "# 8. Choose one model from question four. Split the data into training and test subsets. Build a model with the three variables in the dataset that you think will be good predictors of \"spam\". Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2965cc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_credit:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>spam:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_credit:  word_freq_address:  word_freq_free:  spam:\n",
       "0               0.00                0.64             0.32      1\n",
       "1               0.00                0.28             0.14      1\n",
       "2               0.32                0.00             0.06      1\n",
       "3               0.00                0.00             0.31      1\n",
       "4               0.00                0.00             0.31      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df['word_freq_credit:']\n",
    "df2 = df['word_freq_address:']\n",
    "df3 = df['word_freq_free:']\n",
    "df4 = df['spam']\n",
    "\n",
    "predictors = pd.DataFrame(\n",
    "    {'word_freq_credit:': df1,\n",
    "     'word_freq_address:': df2,\n",
    "     'word_freq_free:': df3,\n",
    "     'spam:':df4\n",
    "    })\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfebe3",
   "metadata": {},
   "source": [
    "### The model I choose for this question: KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3858f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50c1f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_credit:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_free:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_credit:  word_freq_address:  word_freq_free:\n",
       "0               0.00                0.64             0.32\n",
       "1               0.00                0.28             0.14\n",
       "2               0.32                0.00             0.06\n",
       "3               0.00                0.00             0.31\n",
       "4               0.00                0.00             0.31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test subsets\n",
    "\n",
    "y = predictors['spam:']\n",
    "X = predictors.loc[:, predictors.columns != 'spam:']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "450a37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN for Classification (UNSCALED DATA)\n",
      "Training set score: 0.819\n",
      "Test set score: 0.789\n",
      "Mean Cross-Validation, Kfold: 0.794\n"
     ]
    }
   ],
   "source": [
    "# build a model with three variables (unscaled data)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"KNN for Classification (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train))))\n",
    "\n",
    "knn_unscaled = np.mean(cross_val_score(knn, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c280f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data using StandardScalar\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e901064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN for Classification(SCALED DATA)\n",
      "Training set score: 0.819\n",
      "Test set score: 0.789\n",
      "Mean Cross-Validation, Kfold: 0.714\n"
     ]
    }
   ],
   "source": [
    "# Find score using scaled data\n",
    "knn = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for Classification(SCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train))))\n",
    "\n",
    "knn_scaled = np.mean(cross_val_score(knn, X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735a060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN for classification (SCALED DATA)\n",
      "Best Parameter: {'n_neighbors': 4}\n",
      "Best Cross-Validation Score: 0.779\n",
      "Test set Score: 0.775\n"
     ]
    }
   ],
   "source": [
    "#Tune parameters of the knn model and find a particular parameter\n",
    "\n",
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for classification (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(knn_grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81ae52",
   "metadata": {},
   "source": [
    "### Best parameter for KNN is n_neighbors = 4 since it can product best cv score and test set score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd9cad",
   "metadata": {},
   "source": [
    "### Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86fd37fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test set score: 0.775\n"
     ]
    }
   ],
   "source": [
    "# On test data directly\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4).fit(X_train_scaled, y_train)\n",
    "print(\"KNN Test set score: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "best_knn = knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af586f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation, KFold: 0.779\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid,cv=kfold).fit(X_train_scaled, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb48c3",
   "metadata": {},
   "source": [
    "# 9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b9d20",
   "metadata": {},
   "source": [
    "### The model I choose for this question: Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc5bcace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0568aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.846\n",
      "Test set score: 0.785\n",
      "Mean Cross-Validation, Kfold: 0.783\n"
     ]
    }
   ],
   "source": [
    "# build a model with three variables\n",
    "\n",
    "DecTree = DecisionTreeClassifier()\n",
    "DecTree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(DecTree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(DecTree.score(X_test, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(DecTree, X_train, y_train))))\n",
    "\n",
    "DecTree_data = np.mean(cross_val_score(DecTree, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2efd513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Tree Size: {'max_depth': 5}\n",
      "Best Cross-Validation Score: 0.802\n",
      "Test set Score: 0.798\n"
     ]
    }
   ],
   "source": [
    "#Tune parameters of the decision tree model and find particular parameters\n",
    "\n",
    "DecTree_param_grid = {'max_depth': range(1, 50)}\n",
    "DecTree_grid = GridSearchCV(DecisionTreeClassifier(), DecTree_param_grid).fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameter Tree Size: {}\".format(DecTree_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(DecTree_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(DecTree_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f83c47",
   "metadata": {},
   "source": [
    "### Best parameter for Decision tree model is max_depth = 5 since it can product best cv score and test set score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4205255",
   "metadata": {},
   "source": [
    "### Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b020912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree Test set score: 0.798\n"
     ]
    }
   ],
   "source": [
    "# On test data directly\n",
    "\n",
    "DecTree = DecisionTreeClassifier(max_depth=5).fit(X_train, y_train)\n",
    "print(\"decision tree Test set score: {:.3f}\".format(DecTree.score(X_test, y_test)))\n",
    "best_tree = DecTree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2367ce17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation, KFold: 0.802\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "DecTree_param_grid = {'max_depth': range(1, 10)}\n",
    "DecTree_grid = GridSearchCV(DecisionTreeClassifier(), DecTree_param_grid,cv=kfold).fit(X_train, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(DecTree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2badd5f2",
   "metadata": {},
   "source": [
    "###  Did this model predict test data better than your previous model?\n",
    "\n",
    "The decision tree model predicts test data better that the previous model. The test score in KNN model is 0.775, while the test score in decision tree model is 0.798, which is higher. The k-fold cv score in KNN model is 0.779, while the test score in decision tree model is 0.802, which is higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b4b69",
   "metadata": {},
   "source": [
    "## 10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab737c2",
   "metadata": {},
   "source": [
    "### The model I choose for this question: logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fda2a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eef88de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION (UNSCALED DATA)\n",
      "Training set score: 0.739\n",
      "Test set score: 0.729\n",
      "Mean Cross Validation, KFold: 0.739\n"
     ]
    }
   ],
   "source": [
    "#build a model with three variables (unscaled data)\n",
    "logreg = LogisticRegression(penalty = 'none').fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d623ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION (SCALED DATA)\n",
      "Best Parameter: {'C': 0.001}\n",
      "Best Cross-Validation Score: 0.739\n",
      "Test set Score: 0.729\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters of the log model and find particular parameters\n",
    "\n",
    "logreg_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(penalty='none'), logreg_param_grid).fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(logreg_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad11d6",
   "metadata": {},
   "source": [
    "### Best parameter for Logistic regression is C = 0.001 since it can produce best cv score and test set score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b68fb",
   "metadata": {},
   "source": [
    "### Run the model and evaluate prediction error in two ways: A) On test data directly B) using k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a20ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log test set Score: 0.729\n"
     ]
    }
   ],
   "source": [
    "# On test data directly\n",
    "\n",
    "log = LogisticRegression(C=0.001,penalty = 'none').fit(X_train, y_train)\n",
    "print(\"log test set Score: {:.3f}\".format(log.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb4ad774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation, KFold: 0.739\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "log_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_grid = GridSearchCV(LogisticRegression(), log_param_grid,cv=kfold).fit(X_train, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(log, X_train, y_train))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299e7f4",
   "metadata": {},
   "source": [
    "###  Did this model predict test data better than your previous model?\n",
    "\n",
    "The logistic regression model doesn't predict test data better that the previous model. The test score in decision tree model is 0.798, while the test score in logistic regression model is 0.729, which is lower. The k-fold cv score in decision tree model is 0.802, while the test score in logistic regression model is 0.739, which is lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503bdb",
   "metadata": {},
   "source": [
    "## 11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e689ba",
   "metadata": {},
   "source": [
    "### The model I choose for this question: Penalized logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82d1b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED DATA)\n",
      "Training set score: 0.738\n",
      "Test set score: 0.727\n",
      "Mean Cross Validation, KFold: 0.738\n"
     ]
    }
   ],
   "source": [
    "# build a model with three variables (unscaled data)\n",
    "pen_logreg_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(pen_logreg_l1.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(pen_logreg_l1.score(X_test, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(pen_logreg_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e588ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data using standardScalar\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7f19b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\n",
      "Training set score: 0.739\n",
      "Test set score: 0.728\n",
      "Mean Cross Validation, KFold: 0.738\n"
     ]
    }
   ],
   "source": [
    "# Find score using scaled data\n",
    "\n",
    "pen_logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(pen_logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(pen_logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(pen_logreg_scaled_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08499d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENLIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\n",
      "Best Parameter: {'C': 10}\n",
      "Best Cross-Validation Score: 0.739\n",
      "Test set Score: 0.729\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters of the penalized logistic regression model (L1) and find particular parameter\n",
    "\n",
    "pen_logreg_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "pen_logreg_grid_l1 = GridSearchCV(LogisticRegression(penalty='l1', solver = 'liblinear'), pen_logreg_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid_l1.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(pen_logreg_grid_l1.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(pen_logreg_grid_l1.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34609ac9",
   "metadata": {},
   "source": [
    "### Best parameter for penalized logistic regression model is  since it can product best cv score and test set score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f884b",
   "metadata": {},
   "source": [
    "### Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "457439c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log test set Score: 0.729\n"
     ]
    }
   ],
   "source": [
    "#On test data directly\n",
    "\n",
    "penlog = LogisticRegression(C=10,penalty = 'l1',solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "print(\"log test set Score: {:.3f}\".format(penlog.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "661baca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation, KFold: 0.739\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "penlog_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "penlog_grid = GridSearchCV(LogisticRegression(), penlog_param_grid,cv=kfold).fit(X_train_scaled, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(penlog, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6689c1f",
   "metadata": {},
   "source": [
    "###  Did this model predict test data better than your previous model?\n",
    "\n",
    "Both penalized logistic regression model and log regression model without penalization have the same test set score and k-fold cross validation score. So this model doesn't predict data better than the previous model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e0aa1",
   "metadata": {},
   "source": [
    "## 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy. Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ecb0d",
   "metadata": {},
   "source": [
    "### From questions 8-11, my best model would be the decision tree model. And I decide to add these three variables: word_freq_internet:, word_freq_mail, and word_freq_remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a1ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df['word_freq_internet:']\n",
    "df6 = df['word_freq_mail:']\n",
    "df7 = df['word_freq_remove:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98b8364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_money:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>spam:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_money:  word_freq_address:  word_freq_free:  word_freq_internet:  \\\n",
       "0              0.00                0.64             0.32                 0.00   \n",
       "1              0.00                0.28             0.14                 0.07   \n",
       "2              0.32                0.00             0.06                 0.12   \n",
       "3              0.00                0.00             0.31                 0.63   \n",
       "4              0.00                0.00             0.31                 0.63   \n",
       "\n",
       "   word_freq_mail:  word_freq_remove:  spam:  \n",
       "0             0.00               0.00      1  \n",
       "1             0.94               0.21      1  \n",
       "2             0.25               0.19      1  \n",
       "3             0.63               0.31      1  \n",
       "4             0.63               0.31      1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = pd.DataFrame(\n",
    "    {'word_freq_money:': df1,\n",
    "     'word_freq_address:': df2,\n",
    "     'word_freq_free:': df3,\n",
    "     'word_freq_internet:':df5,\n",
    "     'word_freq_mail:':df6,\n",
    "     'word_freq_remove:':df7,\n",
    "     'spam:':df4})\n",
    "new_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7307a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_money:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_money:  word_freq_address:  word_freq_free:  word_freq_internet:  \\\n",
       "0              0.00                0.64             0.32                 0.00   \n",
       "1              0.00                0.28             0.14                 0.07   \n",
       "2              0.32                0.00             0.06                 0.12   \n",
       "3              0.00                0.00             0.31                 0.63   \n",
       "4              0.00                0.00             0.31                 0.63   \n",
       "\n",
       "   word_freq_mail:  word_freq_remove:  \n",
       "0             0.00               0.00  \n",
       "1             0.94               0.21  \n",
       "2             0.25               0.19  \n",
       "3             0.63               0.31  \n",
       "4             0.63               0.31  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and test subsets\n",
    "\n",
    "y = new_model['spam:']\n",
    "X = new_model.loc[:, new_model.columns != 'spam:']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3551e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.929\n",
      "Test set score: 0.825\n",
      "Mean Cross-Validation, Kfold: 0.844\n"
     ]
    }
   ],
   "source": [
    "# build a model with six variables\n",
    "\n",
    "DecTree = DecisionTreeClassifier()\n",
    "DecTree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(DecTree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(DecTree.score(X_test, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(DecTree, X_train, y_train))))\n",
    "\n",
    "DecTree_data = np.mean(cross_val_score(DecTree, X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84660601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Tree Size: {'max_depth': 9}\n",
      "Best Cross-Validation Score: 0.850\n",
      "Test set Score: 0.831\n"
     ]
    }
   ],
   "source": [
    "#Tune parameters of the decision tree model and find particular parameters\n",
    "\n",
    "DecTree_param_grid = {'max_depth': range(1, 50)}\n",
    "DecTree_grid = GridSearchCV(DecisionTreeClassifier(), DecTree_param_grid).fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameter Tree Size: {}\".format(DecTree_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(DecTree_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(DecTree_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297fe08",
   "metadata": {},
   "source": [
    "### Best parameter for Decision tree model is max_depth = 9 since it can product best cv score and test set score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3a0626",
   "metadata": {},
   "source": [
    "### Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8067c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree Test set score: 0.830\n"
     ]
    }
   ],
   "source": [
    "# On test data directly\n",
    "\n",
    "DecTree = DecisionTreeClassifier(max_depth=9).fit(X_train, y_train)\n",
    "print(\"decision tree Test set score: {:.3f}\".format(DecTree.score(X_test, y_test)))\n",
    "best_tree = DecTree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd2235b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation, KFold: 0.848\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "DecTree_param_grid = {'max_depth': range(1, 10)}\n",
    "DecTree_grid = GridSearchCV(DecisionTreeClassifier(), DecTree_param_grid,cv=kfold).fit(X_train, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(DecTree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f3241",
   "metadata": {},
   "source": [
    "### Did this model predict test data better than your previous models?\n",
    "\n",
    "Yes. By adding three new variables, the decision tree model has a larger test set score 0.830, while all the previous models have smaller test set score. Also the the new decision tree model has a larger cross-validation score 0.848, while all the previous models have smaller k-fold cv score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2605976",
   "metadata": {},
   "source": [
    "# 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model. Why did you select this model among all of the models that you ran?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cf626",
   "metadata": {},
   "source": [
    "### KNN model with six variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f4dbea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN for Classification (UNSCALED DATA)\n",
      "Training set score: 0.877\n",
      "Test set score: 0.827\n",
      "Mean Cross-Validation, Kfold: 0.843\n",
      "KNN for Classification(SCALED DATA)\n",
      "Training set score: 0.877\n",
      "Test set score: 0.835\n",
      "Mean Cross-Validation, Kfold: 0.846\n",
      "KNN for classification (SCALED DATA)\n",
      "Best Parameter: {'n_neighbors': 3}\n",
      "Best Cross-Validation Score: 0.850\n",
      "Test set Score: 0.830\n",
      "KNN Test set score: 0.830\n",
      "Mean Cross Validation, KFold: 0.850\n"
     ]
    }
   ],
   "source": [
    "# build a model with three variables (unscaled data)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"KNN for Classification (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train))))\n",
    "\n",
    "knn_unscaled = np.mean(cross_val_score(knn, X_train, y_train))\n",
    "\n",
    "# Preprocessing data using StandardScalar\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Find score using scaled data\n",
    "knn = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for Classification(SCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train))))\n",
    "\n",
    "knn_scaled = np.mean(cross_val_score(knn, X_train_scaled, y_train))\n",
    "\n",
    "#Tune parameters of the knn model and find a particular parameter\n",
    "\n",
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for classification (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(knn_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# On test data directly\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X_train_scaled, y_train)\n",
    "print(\"KNN Test set score: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "best_knn = knn.score(X_test_scaled, y_test)\n",
    "\n",
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid,cv=kfold).fit(X_train_scaled, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713aeb4",
   "metadata": {},
   "source": [
    "### Logistic regression model with six variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "337fda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION (UNSCALED DATA)\n",
      "Training set score: 0.795\n",
      "Test set score: 0.790\n",
      "Mean Cross Validation, KFold: 0.796\n",
      "LOGISTIC REGRESSION (SCALED DATA)\n",
      "Best Parameter: {'C': 0.001}\n",
      "Best Cross-Validation Score: 0.796\n",
      "Test set Score: 0.790\n",
      "log test set Score: 0.790\n",
      "Mean Cross Validation, KFold: 0.796\n"
     ]
    }
   ],
   "source": [
    "#build a model with three variables (unscaled data)\n",
    "logreg = LogisticRegression(penalty = 'none').fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train))))\n",
    "\n",
    "# Tune parameters of the log model and find particular parameters\n",
    "\n",
    "logreg_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(penalty='none'), logreg_param_grid).fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(logreg_grid.score(X_test, y_test)))\n",
    "\n",
    "# On test data directly\n",
    "\n",
    "log = LogisticRegression(C=0.001,penalty = 'none').fit(X_train, y_train)\n",
    "print(\"log test set Score: {:.3f}\".format(log.score(X_test, y_test)))\n",
    "\n",
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "log_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_grid = GridSearchCV(LogisticRegression(), log_param_grid,cv=kfold).fit(X_train, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(log, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375f1cf",
   "metadata": {},
   "source": [
    "### Penalized Logistic regression model with six variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "637d58cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED DATA)\n",
      "Training set score: 0.795\n",
      "Test set score: 0.790\n",
      "Mean Cross Validation, KFold: 0.795\n",
      "PENALIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\n",
      "Training set score: 0.795\n",
      "Test set score: 0.790\n",
      "Mean Cross Validation, KFold: 0.795\n",
      "PENLIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\n",
      "Best Parameter: {'C': 10}\n",
      "Best Cross-Validation Score: 0.796\n",
      "Test set Score: 0.790\n",
      "penlog test set Score: 0.790\n",
      "Mean Cross Validation, KFold: 0.796\n"
     ]
    }
   ],
   "source": [
    "# build a model with three variables (unscaled data)\n",
    "pen_logreg_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(pen_logreg_l1.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(pen_logreg_l1.score(X_test, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(pen_logreg_l1, X_train, y_train))))\n",
    "\n",
    "# preprocessing data using standardScalar\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Find score using scaled data\n",
    "\n",
    "pen_logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\")\n",
    "print(\"Training set score: {:.3f}\".format(pen_logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(pen_logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(pen_logreg_scaled_l1, X_train, y_train))))\n",
    "\n",
    "# Tune parameters of the penalized logistic regression model (L1) and find particular parameter\n",
    "\n",
    "pen_logreg_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "pen_logreg_grid_l1 = GridSearchCV(LogisticRegression(penalty='l1', solver = 'liblinear'), pen_logreg_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION - L1 (SCALED DATA)\")\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid_l1.best_params_))\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(pen_logreg_grid_l1.best_score_))\n",
    "print(\"Test set Score: {:.3f}\".format(pen_logreg_grid_l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "\n",
    "#On test data directly\n",
    "\n",
    "penlog = LogisticRegression(C=1,penalty = 'l1',solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "print(\"penlog test set Score: {:.3f}\".format(penlog.score(X_test_scaled, y_test)))\n",
    "\n",
    "# using k-fold cross-validation\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "penlog_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "penlog_grid = GridSearchCV(LogisticRegression(), penlog_param_grid,cv=kfold).fit(X_train_scaled, y_train)\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(penlog, X_train_scaled, y_train))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5640b",
   "metadata": {},
   "source": [
    "### I still choose the decision tree model since it has the highest cross-validation score 0.848 among all the four models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb228c00",
   "metadata": {},
   "source": [
    "# 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecd135",
   "metadata": {},
   "source": [
    "I may likely add word_freq_money to increase my final model's. If there is any email that asks us to send money and tell us that we can get money easily, the email may be a spam. So it can be a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a0e8b",
   "metadata": {},
   "source": [
    "## 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45955ab",
   "metadata": {},
   "source": [
    "KNN regression model, linear regression model, Ridge regression, Lasso regression, Support Vector Machines(SVM), random forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3a30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0bc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2124931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b687c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71904416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6fab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0981d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a9405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0e9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
